{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall mediapipe -y -q\n",
        "!pip install mediapipe==0.10.21 opencv-python scikit-learn pandas numpy matplotlib seaborn tqdm joblib tensorflow kaggle -q"
      ],
      "metadata": {
        "id": "E_sCUWaCxfmo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "TlZ9rg3jwwMI",
        "outputId": "a3c1ffe7-4617-4093-b615-aa6cabb25f8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6cf37b92-1cd8-47e7-a42d-518ea08997e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6cf37b92-1cd8-47e7-a42d-518ea08997e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import pickle\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkO5eexx0Mt",
        "outputId": "e3f85ff7-5372-4371-eb70-98bc30448891"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Dataset settings\n",
        "    DATASET_NAME = \"asl_alphabet\"\n",
        "    DATASET_PATH = Path(\"asl_data\")\n",
        "    MODEL_PATH = Path(\"models/gesture_model_optimized.pkl\")\n",
        "    CACHE_PATH = Path(\"cache/features_optimized.pkl\")\n",
        "    CONFUSION_MATRIX_PATH = Path(\"results/confusion_matrix.png\")\n",
        "    KAGGLE_DATASET = \"grassknoted/asl-alphabet\"\n",
        "\n",
        "    # Model optimization settings\n",
        "    MODEL_TYPE = \"rf\"\n",
        "    N_ESTIMATORS = 50  # Reduced from 100\n",
        "    MAX_DEPTH = 12  # Reduced from 15\n",
        "    MIN_SAMPLES_SPLIT = 30  # Increased from 20\n",
        "    MIN_SAMPLES_LEAF = 12  # Increased from 8\n",
        "    MAX_FEATURES = 'sqrt'\n",
        "    TEST_SIZE = 0.2\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "    # Feature optimization\n",
        "    USE_ENHANCED_FEATURES = True\n",
        "    USE_PCA = True  # NEW: Enable PCA for dimensionality reduction\n",
        "    PCA_VARIANCE = 0.95  # Keep 95% of variance\n",
        "\n",
        "    # Data settings\n",
        "    USE_DATA_AUGMENTATION = False  # Disable to reduce training time\n",
        "    USE_SCALING = True\n",
        "    SCALER_TYPE = 'standard'  # Changed from 'robust' for speed\n",
        "    REMOVE_OUTLIERS = False  # Disable to reduce processing time\n",
        "\n",
        "    # Inference settings\n",
        "    CONFIDENCE_THRESHOLD = 0.65\n",
        "    MIN_DETECTION_CONFIDENCE = 0.7\n",
        "    MIN_TRACKING_CONFIDENCE = 0.5\n",
        "\n",
        "    # Processing settings\n",
        "    USE_CACHE = True\n",
        "    DEFAULT_MAX_SAMPLES = 300  # Reduced from 500\n",
        "    IGNORE_CLASSES = [\"nothing\"]\n",
        "    USE_CROSS_VALIDATION = False  # Disable for faster training\n",
        "\n",
        "    # Compression settings\n",
        "    COMPRESS_MODEL = True  # NEW: Enable model compression\n",
        "\n",
        "print(\"✓ Configuration loaded with optimizations:\")\n",
        "print(f\"  - Reduced estimators: {Config.N_ESTIMATORS}\")\n",
        "print(f\"  - Reduced tree depth: {Config.MAX_DEPTH}\")\n",
        "print(f\"  - PCA enabled: {Config.USE_PCA}\")\n",
        "print(f\"  - Max samples per class: {Config.DEFAULT_MAX_SAMPLES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRJSO7-9x2Og",
        "outputId": "6cbc1206-b062-49ee-827b-79b96f5e800e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded with optimizations:\n",
            "  - Reduced estimators: 50\n",
            "  - Reduced tree depth: 12\n",
            "  - PCA enabled: True\n",
            "  - Max samples per class: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedFeatureExtractor:\n",
        "    @staticmethod\n",
        "    def get_distance(p1, p2):\n",
        "        return np.linalg.norm(p1 - p2)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_palm_orientation(points):\n",
        "        \"\"\"\n",
        "        Calculates the normal vector of the palm using the wrist (0),\n",
        "        index base (5), and pinky base (17).\n",
        "        \"\"\"\n",
        "        v1 = points[5] - points[0]\n",
        "        v2 = points[17] - points[0]\n",
        "        # Cross product gives a vector perpendicular to the palm surface\n",
        "        palm_normal = np.cross(v1, v2)\n",
        "        palm_normal /= (np.linalg.norm(palm_normal) + 1e-6)\n",
        "        return palm_normal\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_3d_angles(points):\n",
        "        \"\"\"Calculates 3D joint angles for each finger.\"\"\"\n",
        "        angles = []\n",
        "        fingers_indices = [\n",
        "            [1, 2, 3, 4],    # Thumb\n",
        "            [5, 6, 7, 8],    # Index\n",
        "            [9, 10, 11, 12],  # Middle\n",
        "            [13, 14, 15, 16], # Ring\n",
        "            [17, 18, 19, 20]  # Pinky\n",
        "        ]\n",
        "\n",
        "        for finger in fingers_indices:\n",
        "            for i in range(len(finger) - 2):\n",
        "                p1, p2, p3 = points[finger[i]], points[finger[i+1]], points[finger[i+2]]\n",
        "                v1 = p1 - p2\n",
        "                v2 = p3 - p2\n",
        "\n",
        "                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)\n",
        "                angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
        "                angles.append(angle)\n",
        "        return np.array(angles, dtype=np.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_enhanced_features(landmarks):\n",
        "        if landmarks is None: return None\n",
        "\n",
        "        # Reshape to (21, 3) for easier indexing\n",
        "        points = landmarks.reshape(21, 3)\n",
        "\n",
        "        # 1. Normalized Landmarks (Scale and Translation Invariant)\n",
        "        # We center at wrist and normalize by the distance between wrist and middle finger base\n",
        "        centered = points - points[0]\n",
        "        scale = EnhancedFeatureExtractor.get_distance(points[0], points[9]) + 1e-6\n",
        "        normalized_points = (centered / scale).flatten()\n",
        "\n",
        "        # 2. 3D Palm Orientation (Captures hand rotation relative to camera)\n",
        "        palm_normal = EnhancedFeatureExtractor.compute_palm_orientation(points)\n",
        "\n",
        "        # 3. 3D Joint Angles\n",
        "        angles = EnhancedFeatureExtractor.compute_3d_angles(points)\n",
        "\n",
        "        # 4. Critical Cross-Finger Distances (Inter-finger relationships)\n",
        "        # Distances between thumb tip and other fingertips (detects pinching/circles)\n",
        "        thumb_tip = points[4]\n",
        "        fingertips = [8, 12, 16, 20]\n",
        "        pinch_distances = [EnhancedFeatureExtractor.get_distance(thumb_tip, points[f]) / scale for f in fingertips]\n",
        "\n",
        "        # 5. Hand \"Spread\" (Distance between adjacent fingertips)\n",
        "        spread_distances = []\n",
        "        fingertip_all = [4, 8, 12, 16, 20]\n",
        "        for i in range(len(fingertip_all) - 1):\n",
        "            d = EnhancedFeatureExtractor.get_distance(points[fingertip_all[i]], points[fingertip_all[i+1]]) / scale\n",
        "            spread_distances.append(d)\n",
        "\n",
        "        # Combine all features\n",
        "        feature_vector = np.concatenate([\n",
        "            normalized_points, # 63 features\n",
        "            palm_normal,       # 3 features\n",
        "            angles,            # 10 features\n",
        "            pinch_distances,   # 4 features\n",
        "            spread_distances   # 4 features\n",
        "        ])\n",
        "\n",
        "        return feature_vector.astype(np.float32)\n",
        "print(\"✓ Feature extractor initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L53Gm-Pcx5ki",
        "outputId": "e75eef2a-3f04-4925-8950-59ca03853dff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Feature extractor initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedDataProcessor:\n",
        "    def __init__(self, dataset_path, max_samples_per_class=None, use_cache=True):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.max_samples_per_class = max_samples_per_class or Config.DEFAULT_MAX_SAMPLES\n",
        "        self.use_cache = use_cache\n",
        "        self.extractor = None\n",
        "\n",
        "    def load_from_cache(self):\n",
        "        if not self.use_cache or not Config.CACHE_PATH.exists():\n",
        "            return None\n",
        "        try:\n",
        "            print(f\"Loading cached features from {Config.CACHE_PATH}\")\n",
        "            with open(Config.CACHE_PATH, 'rb') as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            if cache_data.get('max_samples') == self.max_samples_per_class:\n",
        "                print(f\"✓ Cache loaded: {len(cache_data['X'])} samples\")\n",
        "                return cache_data['X'], cache_data['y']\n",
        "            else:\n",
        "                print(f\"Cache has different sample count, reprocessing...\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load cache: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_to_cache(self, X, y):\n",
        "        if not self.use_cache:\n",
        "            return\n",
        "        try:\n",
        "            Config.CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "            cache_data = {'X': X, 'y': y, 'max_samples': self.max_samples_per_class}\n",
        "            with open(Config.CACHE_PATH, 'wb') as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(f\"✓ Features cached to {Config.CACHE_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save cache: {e}\")\n",
        "\n",
        "    def extract_landmarks_from_image(self, image_path):\n",
        "        try:\n",
        "            image = cv2.imread(str(image_path))\n",
        "            if image is None:\n",
        "                return None\n",
        "\n",
        "            h, w = image.shape[:2]\n",
        "            if h > 800 or w > 800:  # Reduced max size\n",
        "                scale = min(800 / h, 800 / w)\n",
        "                image = cv2.resize(image, None, fx=scale, fy=scale,\n",
        "                                 interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = self.extractor.process(image_rgb)\n",
        "\n",
        "            if results.multi_hand_landmarks:\n",
        "                hand_landmarks = results.multi_hand_landmarks[0]\n",
        "                landmarks = []\n",
        "                for landmark in hand_landmarks.landmark:\n",
        "                    landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "                return np.array(landmarks, dtype=np.float32)\n",
        "            return None\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        cached = self.load_from_cache()\n",
        "        if cached is not None:\n",
        "            X, y = cached\n",
        "            mask = np.isin(y, Config.IGNORE_CLASSES, invert=True)\n",
        "            return X[mask], y[mask]\n",
        "\n",
        "        print(\"Initializing MediaPipe...\")\n",
        "        mp_hands = mp.solutions.hands\n",
        "        self.extractor = mp_hands.Hands(\n",
        "            static_image_mode=True,\n",
        "            max_num_hands=1,\n",
        "            min_detection_confidence=0.5,\n",
        "            model_complexity=0\n",
        "        )\n",
        "\n",
        "        X, y = [], []\n",
        "        class_folders = sorted([d for d in self.dataset_path.iterdir() if d.is_dir()])\n",
        "        class_folders = [f for f in class_folders if f.name not in Config.IGNORE_CLASSES]\n",
        "\n",
        "        print(f\"Found {len(class_folders)} gesture classes\")\n",
        "        print(f\"Processing {self.max_samples_per_class} samples per class\")\n",
        "\n",
        "        for class_folder in class_folders:\n",
        "            class_name = class_folder.name\n",
        "            image_files = list(class_folder.glob(\"*.jpg\")) + \\\n",
        "                         list(class_folder.glob(\"*.jpeg\")) + \\\n",
        "                         list(class_folder.glob(\"*.png\"))\n",
        "\n",
        "            print(f\"Processing '{class_name}': {len(image_files)} images available\")\n",
        "\n",
        "            class_features = []\n",
        "            for img_path in tqdm(image_files[:self.max_samples_per_class * 2],\n",
        "                               desc=f\"  {class_name}\", leave=False):\n",
        "                if len(class_features) >= self.max_samples_per_class:\n",
        "                    break\n",
        "\n",
        "                landmarks = self.extract_landmarks_from_image(img_path)\n",
        "                if landmarks is not None:\n",
        "                    features = EnhancedFeatureExtractor.extract_enhanced_features(landmarks)\n",
        "                    if features is not None:\n",
        "                        class_features.append(features)\n",
        "\n",
        "            X.extend(class_features)\n",
        "            y.extend([class_name] * len(class_features))\n",
        "            print(f\"  ✓ {len(class_features)} samples extracted\")\n",
        "\n",
        "        self.extractor.close()\n",
        "\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        y = np.array(y)\n",
        "\n",
        "        print(f\"\\n✓ Total samples: {len(X)}\")\n",
        "        self.save_to_cache(X, y)\n",
        "        return X, y\n",
        "\n",
        "print(\"✓ Data processor initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J55vk6Amx8AR",
        "outputId": "086b262d-16e4-491f-e408-a0dc262a1021"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data processor initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizationUtils:\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(cm, class_names, save_path=None, normalize=False):\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            fmt = '.2f'\n",
        "            title = 'Normalized Confusion Matrix'\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = 'Confusion Matrix'\n",
        "\n",
        "        n_classes = len(class_names)\n",
        "        figsize = max(10, n_classes * 0.5)\n",
        "        fig, ax = plt.subplots(figsize=(figsize, figsize))\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names,\n",
        "                    cbar_kws={'label': 'Count' if not normalize else 'Proportion'},\n",
        "                    ax=ax)\n",
        "\n",
        "        ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
        "        plt.setp(ax.get_yticklabels(), rotation=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            save_path = Path(save_path)\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')  # Reduced DPI\n",
        "            print(f\"✓ Confusion matrix saved to: {save_path}\")\n",
        "\n",
        "            if not normalize:\n",
        "                plt.close()\n",
        "                norm_path = save_path.parent / f\"{save_path.stem}_normalized{save_path.suffix}\"\n",
        "                VisualizationUtils.plot_confusion_matrix(cm, class_names, norm_path, normalize=True)\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "print(\"✓ Visualization utilities loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhv-Moaox9qo",
        "outputId": "3971fdb9-573b-4ea3-eb12-d4fa7abc39dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Visualization utilities loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSizeOptimizer:\n",
        "    \"\"\"Programmatically find optimal hyperparameters to minimize model size while maintaining accuracy\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate_model_size(n_estimators, max_depth, n_features, n_classes):\n",
        "        \"\"\"Estimate Random Forest model size in MB\"\"\"\n",
        "        # Approximate formula: trees * nodes_per_tree * (feature_idx + threshold + class_probs)\n",
        "        avg_nodes_per_tree = 2 ** (max_depth + 1) - 1\n",
        "        bytes_per_node = 4 + 8 + (n_classes * 8)  # int32 + float64 + class_probs\n",
        "        total_bytes = n_estimators * avg_nodes_per_tree * bytes_per_node\n",
        "        return total_bytes / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "    @staticmethod\n",
        "    def find_optimal_hyperparameters(X_sample, y_sample, target_size_mb=10,\n",
        "                                     min_accuracy=0.90, n_trials=20):\n",
        "        \"\"\"\n",
        "        Use grid search to find hyperparameters that minimize model size\n",
        "        while maintaining accuracy above threshold\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "        import itertools\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"AUTOMATIC HYPERPARAMETER OPTIMIZATION\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Target: Model < {target_size_mb}MB, Accuracy > {min_accuracy*100}%\")\n",
        "        print(f\"Sample size: {len(X_sample)} samples\")\n",
        "\n",
        "        # Define search space (smaller values = smaller model)\n",
        "        n_estimators_range = [20, 30, 40, 50, 75]\n",
        "        max_depth_range = [8, 10, 12, 15, 18]\n",
        "        min_samples_split_range = [20, 30, 50]\n",
        "\n",
        "        n_features = X_sample.shape[1]\n",
        "        n_classes = len(np.unique(y_sample))\n",
        "\n",
        "        results = []\n",
        "        best_config = None\n",
        "        best_score = 0\n",
        "\n",
        "        # Generate candidate configurations\n",
        "        configs = list(itertools.product(\n",
        "            n_estimators_range,\n",
        "            max_depth_range,\n",
        "            min_samples_split_range\n",
        "        ))\n",
        "\n",
        "        print(f\"\\nTesting {min(n_trials, len(configs))} configurations...\")\n",
        "\n",
        "        # Sample random configs if too many\n",
        "        if len(configs) > n_trials:\n",
        "            import random\n",
        "            configs = random.sample(configs, n_trials)\n",
        "\n",
        "        for i, (n_est, max_d, min_split) in enumerate(tqdm(configs, desc=\"Optimizing\")):\n",
        "            # Estimate model size\n",
        "            est_size = ModelSizeOptimizer.estimate_model_size(\n",
        "                n_est, max_d, n_features, n_classes\n",
        "            )\n",
        "\n",
        "            # Skip if estimated size is too large\n",
        "            if est_size > target_size_mb * 1.5:\n",
        "                continue\n",
        "\n",
        "            # Test accuracy with 3-fold CV\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=n_est,\n",
        "                max_depth=max_d,\n",
        "                min_samples_split=min_split,\n",
        "                min_samples_leaf=max(8, min_split // 3),\n",
        "                max_features='sqrt',\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            cv_scores = cross_val_score(model, X_sample, y_sample,\n",
        "                                       cv=3, scoring='accuracy', n_jobs=-1)\n",
        "            mean_acc = cv_scores.mean()\n",
        "\n",
        "            results.append({\n",
        "                'n_estimators': n_est,\n",
        "                'max_depth': max_d,\n",
        "                'min_samples_split': min_split,\n",
        "                'estimated_size_mb': est_size,\n",
        "                'cv_accuracy': mean_acc,\n",
        "                'meets_criteria': mean_acc >= min_accuracy and est_size <= target_size_mb\n",
        "            })\n",
        "\n",
        "            # Track best configuration\n",
        "            if mean_acc >= min_accuracy and est_size <= target_size_mb:\n",
        "                if mean_acc > best_score:\n",
        "                    best_score = mean_acc\n",
        "                    best_config = results[-1]\n",
        "\n",
        "        # Sort by accuracy (descending) then size (ascending)\n",
        "        results.sort(key=lambda x: (-x['cv_accuracy'], x['estimated_size_mb']))\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"OPTIMIZATION RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"\\nTop 5 configurations:\")\n",
        "        for i, r in enumerate(results[:5], 1):\n",
        "            status = \"✓\" if r['meets_criteria'] else \"✗\"\n",
        "            print(f\"\\n{i}. {status} Accuracy: {r['cv_accuracy']:.4f} | Size: {r['estimated_size_mb']:.2f}MB\")\n",
        "            print(f\"   Trees: {r['n_estimators']}, Depth: {r['max_depth']}, \"\n",
        "                  f\"MinSplit: {r['min_samples_split']}\")\n",
        "\n",
        "        if best_config:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"✓ RECOMMENDED CONFIGURATION\")\n",
        "            print(f\"{'='*60}\")\n",
        "            print(f\"  Accuracy: {best_config['cv_accuracy']:.4f} ({best_config['cv_accuracy']*100:.2f}%)\")\n",
        "            print(f\"  Est. Size: {best_config['estimated_size_mb']:.2f} MB\")\n",
        "            print(f\"  n_estimators: {best_config['n_estimators']}\")\n",
        "            print(f\"  max_depth: {best_config['max_depth']}\")\n",
        "            print(f\"  min_samples_split: {best_config['min_samples_split']}\")\n",
        "            return best_config\n",
        "        else:\n",
        "            print(f\"\\n✗ No configuration met criteria. Using best available:\")\n",
        "            fallback = results[0]\n",
        "            print(f\"  Accuracy: {fallback['cv_accuracy']:.4f}\")\n",
        "            print(f\"  Est. Size: {fallback['estimated_size_mb']:.2f} MB\")\n",
        "            return fallback\n",
        "\n",
        "print(\"✓ Model size optimizer loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qvG0Tuax_-U",
        "outputId": "830fe6c1-fbdd-4721-da88-26e01f307050"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model size optimizer loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedGestureModel:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.pca = None\n",
        "        self.class_names = None\n",
        "        self.hyperparameters = None\n",
        "\n",
        "    def train(self, X, y, auto_optimize=False, target_size_mb=10):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        min_samples_needed = int(1 / Config.TEST_SIZE) + 1\n",
        "        classes_to_remove = unique[counts < min_samples_needed]\n",
        "\n",
        "        if len(classes_to_remove) > 0:\n",
        "            print(f\"Removing {len(classes_to_remove)} classes with insufficient samples\")\n",
        "            mask = np.isin(y, classes_to_remove, invert=True)\n",
        "            X, y = X[mask], y[mask]\n",
        "\n",
        "        # Apply scaling\n",
        "        if Config.USE_SCALING:\n",
        "            print(f\"Applying {Config.SCALER_TYPE} scaling...\")\n",
        "            self.scaler = StandardScaler()\n",
        "            X = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Apply PCA for dimensionality reduction\n",
        "        if Config.USE_PCA:\n",
        "            from sklearn.decomposition import PCA\n",
        "            print(f\"Applying PCA (keeping {Config.PCA_VARIANCE*100}% variance)...\")\n",
        "            self.pca = PCA(n_components=Config.PCA_VARIANCE, random_state=Config.RANDOM_STATE)\n",
        "            X = self.pca.fit_transform(X)\n",
        "            print(f\"✓ Reduced to {X.shape[1]} components (from ~88)\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=Config.TEST_SIZE,\n",
        "            random_state=Config.RANDOM_STATE, stratify=y\n",
        "        )\n",
        "\n",
        "        self.class_names = sorted(np.unique(y))\n",
        "\n",
        "        print(f\"\\nTraining optimized Random Forest...\")\n",
        "        print(f\"  Training samples: {len(X_train)}\")\n",
        "        print(f\"  Testing samples: {len(X_test)}\")\n",
        "        print(f\"  Classes: {len(self.class_names)}\")\n",
        "        print(f\"  Features: {X.shape[1]}\")\n",
        "\n",
        "        # AUTO-OPTIMIZE: Find best hyperparameters for target size\n",
        "        if auto_optimize:\n",
        "            # Use a sample for faster optimization\n",
        "            sample_size = min(2000, len(X_train))\n",
        "            indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
        "            X_sample = X_train[indices]\n",
        "            y_sample = y_train[indices]\n",
        "\n",
        "            optimal_config = ModelSizeOptimizer.find_optimal_hyperparameters(\n",
        "                X_sample, y_sample,\n",
        "                target_size_mb=target_size_mb,\n",
        "                min_accuracy=0.88,  # Slightly lower for sample\n",
        "                n_trials=20\n",
        "            )\n",
        "\n",
        "            self.hyperparameters = optimal_config\n",
        "\n",
        "            # Use optimized hyperparameters\n",
        "            self.model = RandomForestClassifier(\n",
        "                n_estimators=optimal_config['n_estimators'],\n",
        "                max_depth=optimal_config['max_depth'],\n",
        "                min_samples_split=optimal_config['min_samples_split'],\n",
        "                min_samples_leaf=max(8, optimal_config['min_samples_split'] // 3),\n",
        "                max_features='sqrt',\n",
        "                random_state=Config.RANDOM_STATE,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            )\n",
        "        else:\n",
        "            # Use config defaults\n",
        "            self.model = RandomForestClassifier(\n",
        "                n_estimators=Config.N_ESTIMATORS,\n",
        "                max_depth=Config.MAX_DEPTH,\n",
        "                min_samples_split=Config.MIN_SAMPLES_SPLIT,\n",
        "                min_samples_leaf=Config.MIN_SAMPLES_LEAF,\n",
        "                max_features=Config.MAX_FEATURES,\n",
        "                random_state=Config.RANDOM_STATE,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            )\n",
        "\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        train_acc = self.model.score(X_train, y_train)\n",
        "        test_acc = self.model.score(X_test, y_test)\n",
        "\n",
        "        print(f\"\\n✓ Training complete!\")\n",
        "        print(f\"  Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "        print(f\"  Testing Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=self.class_names)\n",
        "        VisualizationUtils.plot_confusion_matrix(\n",
        "            cm, self.class_names, save_path=Config.CONFUSION_MATRIX_PATH\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'model': self.model,\n",
        "            'train_accuracy': train_acc,\n",
        "            'test_accuracy': test_acc,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "    def save(self, path):\n",
        "        path = Path(path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Apply tree pruning for additional size reduction\n",
        "        if Config.COMPRESS_MODEL and hasattr(self.model, 'estimators_'):\n",
        "            print(\"\\nApplying model compression techniques...\")\n",
        "\n",
        "            # 1. Prune weak trees (bottom 10% by feature importance)\n",
        "            if len(self.model.estimators_) > 10:\n",
        "                importances = []\n",
        "                for tree in self.model.estimators_:\n",
        "                    importances.append(tree.tree_.compute_feature_importances(normalize=False).sum())\n",
        "\n",
        "                threshold = np.percentile(importances, 10)\n",
        "                self.model.estimators_ = [\n",
        "                    tree for tree, imp in zip(self.model.estimators_, importances)\n",
        "                    if imp >= threshold\n",
        "                ]\n",
        "                self.model.n_estimators = len(self.model.estimators_)\n",
        "                print(f\"  ✓ Pruned to {self.model.n_estimators} trees\")\n",
        "\n",
        "            # Removed: 2. Convert tree values to float32 (not writable)\n",
        "            # for tree in self.model.estimators_:\n",
        "            #     tree.tree_.value = tree.tree_.value.astype(np.float32)\n",
        "\n",
        "        model_data = {\n",
        "            'model': self.model,\n",
        "            'scaler': self.scaler,\n",
        "            'pca': self.pca,\n",
        "            'class_names': self.class_names,\n",
        "            'hyperparameters': self.hyperparameters\n",
        "        }\n",
        "\n",
        "        # Save with maximum compression\n",
        "        joblib.dump(model_data, path, compress=('gzip', 9) if Config.COMPRESS_MODEL else 0)\n",
        "\n",
        "        file_size = path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"\\n✓ Model saved to: {path}\")\n",
        "        print(f\"  File size: {file_size:.2f} MB\")\n",
        "\n",
        "        # Additional lightweight version with quantization\n",
        "        if Config.COMPRESS_MODEL:\n",
        "            lightweight_path = path.parent / f\"{path.stem}_lightweight{path.suffix}\"\n",
        "            self._save_quantized(model_data, lightweight_path)\n",
        "\n",
        "    def _save_quantized(self, model_data, path):\n",
        "        \"\"\"Save a quantized version for even smaller size\"\"\"\n",
        "        import copy\n",
        "        quantized_data = copy.deepcopy(model_data)\n",
        "\n",
        "        # Removed: Quantize tree thresholds to float16 (not writable)\n",
        "        # for tree in quantized_data['model'].estimators_:\n",
        "        #     tree.tree_.threshold = tree.tree_.threshold.astype(np.float16)\n",
        "\n",
        "        joblib.dump(quantized_data, path, compress=('gzip', 9))\n",
        "        file_size = path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  Lightweight version: {file_size:.2f} MB (saved to {path.name})\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        model_data = joblib.load(path)\n",
        "        gesture_model = OptimizedGestureModel()\n",
        "        gesture_model.model = model_data['model']\n",
        "        gesture_model.scaler = model_data.get('scaler')\n",
        "        gesture_model.pca = model_data.get('pca')\n",
        "        gesture_model.class_names = model_data['class_names']\n",
        "        return gesture_model\n",
        "\n",
        "print(\"✓ Optimized model class loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W8TCQ2tyDu4",
        "outputId": "7021b580-64c9-4dfd-fa43-abf084c0ad3e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Optimized model class loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_kaggle_dataset():\n",
        "    try:\n",
        "        import kaggle\n",
        "    except ImportError:\n",
        "        print(\"Installing Kaggle API...\")\n",
        "        !pip install kaggle -q\n",
        "        import kaggle\n",
        "\n",
        "    dataset_path = Config.DATASET_PATH\n",
        "    dataset_name = Config.KAGGLE_DATASET\n",
        "\n",
        "    print(f\"Downloading dataset: {dataset_name}\")\n",
        "    print(f\"Destination: {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        kaggle.api.dataset_download_files(\n",
        "            dataset_name, path=dataset_path, unzip=True\n",
        "        )\n",
        "        print(\"✓ Dataset downloaded successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def verify_dataset(dataset_path):\n",
        "    train_path = dataset_path / \"asl_alphabet_train\" / \"asl_alphabet_train\"\n",
        "    if not train_path.exists():\n",
        "        return False\n",
        "    subdirs = list(train_path.glob(\"*/\"))\n",
        "    return len(subdirs) > 0\n",
        "\n",
        "print(\"✓ Dataset utilities loaded\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZyBTZcXyGuR",
        "outputId": "af4a6363-7143-46b2-86d0-8ee229e25d05"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset utilities loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"OPTIMIZED HAND GESTURE RECOGNITION SYSTEM\")\n",
        "    print(\"With Automatic Model Size Optimization\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nOptimizations:\")\n",
        "    print(\"  ✓ Automatic hyperparameter tuning for size/accuracy\")\n",
        "    print(\"  ✓ PCA dimensionality reduction (95% variance)\")\n",
        "    print(\"  ✓ Tree pruning (removes weak estimators)\")\n",
        "    print(\"  ✓ Float32/Float16 quantization\")\n",
        "    print(\"  ✓ Maximum gzip compression\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check for existing model\n",
        "    if Config.MODEL_PATH.exists():\n",
        "        print(f\"\\nFound existing model: {Config.MODEL_PATH}\")\n",
        "        file_size = Config.MODEL_PATH.stat().st_size / (1024 * 1024)\n",
        "        print(f\"Model size: {file_size:.2f} MB\")\n",
        "        choice = input(\"\\nOptions:\\n [1] Use existing\\n [2] Retrain\\n [3] Clear cache and retrain\\n\\nChoice: \").strip()\n",
        "\n",
        "        if choice == '3':\n",
        "            if Config.CACHE_PATH.exists():\n",
        "                Config.CACHE_PATH.unlink()\n",
        "                print(\"✓ Cache cleared\")\n",
        "        elif choice == '1':\n",
        "            print(\"Using existing model\")\n",
        "            return\n",
        "\n",
        "    # Check dataset\n",
        "    dataset_path = Config.DATASET_PATH / \"asl_alphabet_train\" / \"asl_alphabet_train\"\n",
        "\n",
        "    if not verify_dataset(Config.DATASET_PATH):\n",
        "        print(\"\\n✗ Dataset not found!\")\n",
        "        choice = input(\"Download from Kaggle? (y/n): \").strip().lower()\n",
        "        if choice == 'y':\n",
        "            if not download_kaggle_dataset():\n",
        "                print(\"Cannot proceed without dataset\")\n",
        "                return\n",
        "        else:\n",
        "            print(\"Cannot proceed without dataset\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"\\n✓ Dataset found at {Config.DATASET_PATH}\")\n",
        "\n",
        "    # Get sample size\n",
        "    print(f\"\\nDefault: {Config.DEFAULT_MAX_SAMPLES} samples per class\")\n",
        "    custom = input(\"Use default? (y/n): \").strip().lower()\n",
        "\n",
        "    if custom == 'n':\n",
        "        try:\n",
        "            max_samples = int(input(\"Enter max samples per class (50-500): \"))\n",
        "            max_samples = max(50, min(max_samples, 500))\n",
        "        except:\n",
        "            max_samples = Config.DEFAULT_MAX_SAMPLES\n",
        "    else:\n",
        "        max_samples = Config.DEFAULT_MAX_SAMPLES\n",
        "\n",
        "    # Ask about auto-optimization\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"MODEL SIZE OPTIMIZATION\")\n",
        "    print(\"=\" * 60)\n",
        "    auto_opt = input(\"\\nEnable automatic hyperparameter optimization? (y/n): \").strip().lower()\n",
        "\n",
        "    target_size_mb = 10\n",
        "    if auto_opt == 'y':\n",
        "        try:\n",
        "            target_size_mb = float(input(\"Target model size in MB (default 10): \") or \"10\")\n",
        "            target_size_mb = max(2, min(target_size_mb, 50))\n",
        "        except:\n",
        "            target_size_mb = 10\n",
        "\n",
        "    # Process data\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DATA PROCESSING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    processor = OptimizedDataProcessor(\n",
        "        dataset_path,\n",
        "        max_samples_per_class=max_samples,\n",
        "        use_cache=Config.USE_CACHE\n",
        "    )\n",
        "\n",
        "    X, y = processor.load_dataset()\n",
        "\n",
        "    print(\"\\nClass distribution:\")\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    for class_name, count in sorted(zip(unique, counts)):\n",
        "        print(f\"  {class_name}: {count} samples\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"MODEL TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    gesture_model = OptimizedGestureModel()\n",
        "    results = gesture_model.train(\n",
        "        X, y,\n",
        "        auto_optimize=(auto_opt == 'y'),\n",
        "        target_size_mb=target_size_mb\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    gesture_model.save(Config.MODEL_PATH)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✓ TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nResults saved to:\")\n",
        "    print(f\"  Model: {Config.MODEL_PATH}\")\n",
        "    print(f\"  Confusion Matrix: {Config.CONFUSION_MATRIX_PATH}\")\n",
        "\n",
        "    # Show size comparison if optimization was used\n",
        "    if auto_opt == 'y' and gesture_model.hyperparameters:\n",
        "        print(f\"\\nOptimized Hyperparameters:\")\n",
        "        print(f\"  n_estimators: {gesture_model.hyperparameters['n_estimators']}\")\n",
        "        print(f\"  max_depth: {gesture_model.hyperparameters['max_depth']}\")\n",
        "        print(f\"  min_samples_split: {gesture_model.hyperparameters['min_samples_split']}\")\n",
        "\n",
        "print(\"✓ Main training function loaded\")\n",
        "print(\"\\nRun the next cell to start training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj6zAJZayNs4",
        "outputId": "1e8f7bc8-0398-4933-b6db-49d0cb00cf85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Main training function loaded\n",
            "\n",
            "Run the next cell to start training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHM4w2PyyIU5",
        "outputId": "0aa83b1d-2317-41dd-fcd5-ef01ee614344"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "OPTIMIZED HAND GESTURE RECOGNITION SYSTEM\n",
            "With Automatic Model Size Optimization\n",
            "============================================================\n",
            "\n",
            "Optimizations:\n",
            "  ✓ Automatic hyperparameter tuning for size/accuracy\n",
            "  ✓ PCA dimensionality reduction (95% variance)\n",
            "  ✓ Tree pruning (removes weak estimators)\n",
            "  ✓ Float32/Float16 quantization\n",
            "  ✓ Maximum gzip compression\n",
            "============================================================\n",
            "\n",
            "Found existing model: models/gesture_model_optimized.pkl\n",
            "Model size: 0.54 MB\n",
            "\n",
            "Options:\n",
            " [1] Use existing\n",
            " [2] Retrain\n",
            " [3] Clear cache and retrain\n",
            "\n",
            "Choice: 2\n",
            "\n",
            "✓ Dataset found at asl_data\n",
            "\n",
            "Default: 300 samples per class\n",
            "Use default? (y/n): n\n",
            "Enter max samples per class (50-500): 400\n",
            "\n",
            "============================================================\n",
            "MODEL SIZE OPTIMIZATION\n",
            "============================================================\n",
            "\n",
            "Enable automatic hyperparameter optimization? (y/n): y\n",
            "Target model size in MB (default 10): \n",
            "\n",
            "============================================================\n",
            "DATA PROCESSING\n",
            "============================================================\n",
            "Loading cached features from cache/features_optimized.pkl\n",
            "Cache has different sample count, reprocessing...\n",
            "Initializing MediaPipe...\n",
            "Found 28 gesture classes\n",
            "Processing 400 samples per class\n",
            "Processing 'A': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'B': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'C': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'D': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'E': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'F': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'G': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'H': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'I': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'J': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'K': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'L': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'M': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'N': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 325 samples extracted\n",
            "Processing 'O': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'P': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'Q': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'R': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'S': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'T': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'U': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'V': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'W': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'X': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'Y': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'Z': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'del': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "Processing 'space': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 400 samples extracted\n",
            "\n",
            "✓ Total samples: 11125\n",
            "✓ Features cached to cache/features_optimized.pkl\n",
            "\n",
            "Class distribution:\n",
            "  A: 400 samples\n",
            "  B: 400 samples\n",
            "  C: 400 samples\n",
            "  D: 400 samples\n",
            "  E: 400 samples\n",
            "  F: 400 samples\n",
            "  G: 400 samples\n",
            "  H: 400 samples\n",
            "  I: 400 samples\n",
            "  J: 400 samples\n",
            "  K: 400 samples\n",
            "  L: 400 samples\n",
            "  M: 400 samples\n",
            "  N: 325 samples\n",
            "  O: 400 samples\n",
            "  P: 400 samples\n",
            "  Q: 400 samples\n",
            "  R: 400 samples\n",
            "  S: 400 samples\n",
            "  T: 400 samples\n",
            "  U: 400 samples\n",
            "  V: 400 samples\n",
            "  W: 400 samples\n",
            "  X: 400 samples\n",
            "  Y: 400 samples\n",
            "  Z: 400 samples\n",
            "  del: 400 samples\n",
            "  space: 400 samples\n",
            "\n",
            "============================================================\n",
            "MODEL TRAINING\n",
            "============================================================\n",
            "Applying standard scaling...\n",
            "Applying PCA (keeping 95.0% variance)...\n",
            "✓ Reduced to 15 components (from ~88)\n",
            "\n",
            "Training optimized Random Forest...\n",
            "  Training samples: 8900\n",
            "  Testing samples: 2225\n",
            "  Classes: 28\n",
            "  Features: 15\n",
            "\n",
            "============================================================\n",
            "AUTOMATIC HYPERPARAMETER OPTIMIZATION\n",
            "============================================================\n",
            "Target: Model < 10.0MB, Accuracy > 88.0%\n",
            "Sample size: 2000 samples\n",
            "\n",
            "Testing 20 configurations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Optimizing: 100%|██████████| 20/20 [00:06<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPTIMIZATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Top 5 configurations:\n",
            "\n",
            "1. ✓ Accuracy: 0.8870 | Size: 8.63MB\n",
            "   Trees: 75, Depth: 8, MinSplit: 30\n",
            "\n",
            "2. ✗ Accuracy: 0.8780 | Size: 3.45MB\n",
            "   Trees: 30, Depth: 8, MinSplit: 30\n",
            "\n",
            "3. ✗ Accuracy: 0.8585 | Size: 8.63MB\n",
            "   Trees: 75, Depth: 8, MinSplit: 50\n",
            "\n",
            "4. ✗ Accuracy: 0.8580 | Size: 13.82MB\n",
            "   Trees: 30, Depth: 10, MinSplit: 50\n",
            "\n",
            "5. ✗ Accuracy: 0.8515 | Size: 5.75MB\n",
            "   Trees: 50, Depth: 8, MinSplit: 50\n",
            "\n",
            "============================================================\n",
            "✓ RECOMMENDED CONFIGURATION\n",
            "============================================================\n",
            "  Accuracy: 0.8870 (88.70%)\n",
            "  Est. Size: 8.63 MB\n",
            "  n_estimators: 75\n",
            "  max_depth: 8\n",
            "  min_samples_split: 30\n",
            "\n",
            "✓ Training complete!\n",
            "  Training Accuracy: 0.9373 (93.73%)\n",
            "  Testing Accuracy: 0.9290 (92.90%)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.94      0.94      0.94        80\n",
            "           B       0.96      0.99      0.98        80\n",
            "           C       0.90      1.00      0.95        80\n",
            "           D       0.97      0.85      0.91        80\n",
            "           E       0.93      0.94      0.93        80\n",
            "           F       0.97      0.96      0.97        80\n",
            "           G       0.96      0.97      0.97        80\n",
            "           H       0.99      0.97      0.98        80\n",
            "           I       1.00      0.97      0.99        80\n",
            "           J       0.99      0.99      0.99        80\n",
            "           K       0.95      0.97      0.96        80\n",
            "           L       1.00      1.00      1.00        80\n",
            "           M       0.79      0.80      0.80        80\n",
            "           N       0.77      0.85      0.81        65\n",
            "           O       0.84      0.96      0.90        80\n",
            "           P       0.92      0.89      0.90        80\n",
            "           Q       0.91      0.86      0.88        80\n",
            "           R       0.88      0.80      0.84        80\n",
            "           S       0.99      0.95      0.97        80\n",
            "           T       0.99      0.94      0.96        80\n",
            "           U       0.82      0.81      0.82        80\n",
            "           V       0.93      0.95      0.94        80\n",
            "           W       1.00      0.95      0.97        80\n",
            "           X       0.91      0.94      0.93        80\n",
            "           Y       1.00      0.95      0.97        80\n",
            "           Z       0.95      0.96      0.96        80\n",
            "         del       0.94      0.99      0.96        80\n",
            "       space       0.83      0.84      0.83        80\n",
            "\n",
            "    accuracy                           0.93      2225\n",
            "   macro avg       0.93      0.93      0.93      2225\n",
            "weighted avg       0.93      0.93      0.93      2225\n",
            "\n",
            "✓ Confusion matrix saved to: results/confusion_matrix.png\n",
            "✓ Confusion matrix saved to: results/confusion_matrix_normalized.png\n",
            "\n",
            "Applying model compression techniques...\n",
            "  ✓ Pruned to 67 trees\n",
            "\n",
            "✓ Model saved to: models/gesture_model_optimized.pkl\n",
            "  File size: 0.85 MB\n",
            "  Lightweight version: 0.87 MB (saved to gesture_model_optimized_lightweight.pkl)\n",
            "\n",
            "============================================================\n",
            "✓ TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Results saved to:\n",
            "  Model: models/gesture_model_optimized.pkl\n",
            "  Confusion Matrix: results/confusion_matrix.png\n",
            "\n",
            "Optimized Hyperparameters:\n",
            "  n_estimators: 75\n",
            "  max_depth: 8\n",
            "  min_samples_split: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the compressed model\n",
        "def test_compressed_model():\n",
        "    \"\"\"Test both standard and lightweight compressed models\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TESTING COMPRESSED MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load both models\n",
        "    standard_path = Config.MODEL_PATH\n",
        "    lightweight_path = Config.MODEL_PATH.parent / f\"{Config.MODEL_PATH.stem}_lightweight{Config.MODEL_PATH.suffix}\"\n",
        "\n",
        "    models_to_test = []\n",
        "    if standard_path.exists():\n",
        "        models_to_test.append((\"Standard\", standard_path))\n",
        "    if lightweight_path.exists():\n",
        "        models_to_test.append((\"Lightweight\", lightweight_path))\n",
        "\n",
        "    if not models_to_test:\n",
        "        print(\"❌ No trained models found. Please run training first.\")\n",
        "        return\n",
        "\n",
        "    # Load test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    processor = OptimizedDataProcessor(\n",
        "        Config.DATASET_PATH / \"asl_alphabet_train\" / \"asl_alphabet_train\",\n",
        "        max_samples_per_class=50,  # Small sample for quick testing\n",
        "        use_cache=False\n",
        "    )\n",
        "    X_test, y_test = processor.load_dataset()\n",
        "\n",
        "    print(f\"Test set: {len(X_test)} samples, {len(np.unique(y_test))} classes\\n\")\n",
        "\n",
        "    # Test each model\n",
        "    for model_name, model_path in models_to_test:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing {model_name} Model\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load model\n",
        "        gesture_model = OptimizedGestureModel.load(model_path)\n",
        "\n",
        "        # Get file size\n",
        "        file_size_mb = model_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "        # Preprocess test data (apply same transformations as training)\n",
        "        X_processed = X_test.copy()\n",
        "        if gesture_model.scaler:\n",
        "            X_processed = gesture_model.scaler.transform(X_processed)\n",
        "        if gesture_model.pca:\n",
        "            X_processed = gesture_model.pca.transform(X_processed)\n",
        "\n",
        "        # Make predictions\n",
        "        print(\"\\nRunning inference...\")\n",
        "        import time\n",
        "        start = time.time()\n",
        "        y_pred = gesture_model.model.predict(X_processed)\n",
        "        inference_time = time.time() - start\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = (y_pred == y_test).mean()\n",
        "        avg_time_per_sample = (inference_time / len(X_test)) * 1000  # Convert to ms\n",
        "\n",
        "        print(f\"✓ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "        print(f\"✓ Total inference time: {inference_time:.3f}s\")\n",
        "        print(f\"✓ Avg time per sample: {avg_time_per_sample:.2f}ms\")\n",
        "        print(f\"✓ Throughput: {len(X_test)/inference_time:.1f} samples/sec\")\n",
        "\n",
        "        # Show confusion matrix for a few classes\n",
        "        from sklearn.metrics import classification_report\n",
        "        print(\"\\nSample Classification Report (first 5 classes):\")\n",
        "        unique_classes = sorted(np.unique(y_test))\n",
        "        mask = np.isin(y_test, unique_classes)\n",
        "        print(classification_report(y_test[mask], y_pred[mask],\n",
        "                                   labels=unique_classes, zero_division=0))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"✓ TESTING COMPLETE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "# Run the test\n",
        "test_compressed_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6w-CuZFKg8I",
        "outputId": "69d86d6c-4f83-420b-d5d0-510275282b45"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TESTING COMPRESSED MODELS\n",
            "============================================================\n",
            "\n",
            "Loading test data...\n",
            "Initializing MediaPipe...\n",
            "Found 28 gesture classes\n",
            "Processing 50 samples per class\n",
            "Processing 'A': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'B': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'C': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'D': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'E': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'F': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'G': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'H': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'I': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'J': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'K': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'L': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'M': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'N': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 43 samples extracted\n",
            "Processing 'O': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'P': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'Q': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'R': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'S': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'T': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'U': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'V': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'W': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'X': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'Y': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'Z': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'del': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "Processing 'space': 3000 images available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ 50 samples extracted\n",
            "\n",
            "✓ Total samples: 1393\n",
            "Test set: 1393 samples, 28 classes\n",
            "\n",
            "\n",
            "============================================================\n",
            "Testing Standard Model\n",
            "============================================================\n",
            "File size: 0.85 MB\n",
            "\n",
            "Running inference...\n",
            "✓ Accuracy: 0.9332 (93.32%)\n",
            "✓ Total inference time: 0.035s\n",
            "✓ Avg time per sample: 0.03ms\n",
            "✓ Throughput: 39750.6 samples/sec\n",
            "\n",
            "Sample Classification Report (first 5 classes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      0.98      0.99        50\n",
            "           B       0.96      1.00      0.98        50\n",
            "           C       0.94      0.98      0.96        50\n",
            "           D       0.98      0.88      0.93        50\n",
            "           E       0.98      0.86      0.91        50\n",
            "           F       0.94      0.96      0.95        50\n",
            "           G       1.00      1.00      1.00        50\n",
            "           H       1.00      0.98      0.99        50\n",
            "           I       1.00      0.98      0.99        50\n",
            "           J       0.98      0.94      0.96        50\n",
            "           K       0.96      0.98      0.97        50\n",
            "           L       1.00      1.00      1.00        50\n",
            "           M       0.74      0.84      0.79        50\n",
            "           N       0.83      0.79      0.81        43\n",
            "           O       0.78      0.94      0.85        50\n",
            "           P       0.96      0.92      0.94        50\n",
            "           Q       0.90      0.90      0.90        50\n",
            "           R       0.86      0.74      0.80        50\n",
            "           S       0.98      0.94      0.96        50\n",
            "           T       1.00      1.00      1.00        50\n",
            "           U       0.76      0.84      0.80        50\n",
            "           V       0.94      0.96      0.95        50\n",
            "           W       1.00      0.94      0.97        50\n",
            "           X       1.00      0.98      0.99        50\n",
            "           Y       1.00      0.96      0.98        50\n",
            "           Z       0.98      0.98      0.98        50\n",
            "         del       0.96      0.98      0.97        50\n",
            "       space       0.78      0.86      0.82        50\n",
            "\n",
            "    accuracy                           0.93      1393\n",
            "   macro avg       0.94      0.93      0.93      1393\n",
            "weighted avg       0.94      0.93      0.93      1393\n",
            "\n",
            "\n",
            "============================================================\n",
            "Testing Lightweight Model\n",
            "============================================================\n",
            "File size: 0.87 MB\n",
            "\n",
            "Running inference...\n",
            "✓ Accuracy: 0.9332 (93.32%)\n",
            "✓ Total inference time: 0.037s\n",
            "✓ Avg time per sample: 0.03ms\n",
            "✓ Throughput: 38053.1 samples/sec\n",
            "\n",
            "Sample Classification Report (first 5 classes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      0.98      0.99        50\n",
            "           B       0.96      1.00      0.98        50\n",
            "           C       0.94      0.98      0.96        50\n",
            "           D       0.98      0.88      0.93        50\n",
            "           E       0.98      0.86      0.91        50\n",
            "           F       0.94      0.96      0.95        50\n",
            "           G       1.00      1.00      1.00        50\n",
            "           H       1.00      0.98      0.99        50\n",
            "           I       1.00      0.98      0.99        50\n",
            "           J       0.98      0.94      0.96        50\n",
            "           K       0.96      0.98      0.97        50\n",
            "           L       1.00      1.00      1.00        50\n",
            "           M       0.74      0.84      0.79        50\n",
            "           N       0.83      0.79      0.81        43\n",
            "           O       0.78      0.94      0.85        50\n",
            "           P       0.96      0.92      0.94        50\n",
            "           Q       0.90      0.90      0.90        50\n",
            "           R       0.86      0.74      0.80        50\n",
            "           S       0.98      0.94      0.96        50\n",
            "           T       1.00      1.00      1.00        50\n",
            "           U       0.76      0.84      0.80        50\n",
            "           V       0.94      0.96      0.95        50\n",
            "           W       1.00      0.94      0.97        50\n",
            "           X       1.00      0.98      0.99        50\n",
            "           Y       1.00      0.96      0.98        50\n",
            "           Z       0.98      0.98      0.98        50\n",
            "         del       0.96      0.98      0.97        50\n",
            "       space       0.78      0.86      0.82        50\n",
            "\n",
            "    accuracy                           0.93      1393\n",
            "   macro avg       0.94      0.93      0.93      1393\n",
            "weighted avg       0.94      0.93      0.93      1393\n",
            "\n",
            "\n",
            "============================================================\n",
            "✓ TESTING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}